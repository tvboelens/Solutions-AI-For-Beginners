{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the data and unpack the zip file. The dataset is very large, so downloading takes quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    if not os.path.exists('HollywoodHeads.zip'):\n",
    "        print('Downloading dataset, this might take a while')\n",
    "        !wget https://www.di.ens.fr/willow/research/headdetection/release/HollywoodHeads.zip\n",
    "    with zipfile.ZipFile('HollywoodHeads.zip') as file:\n",
    "        print('Unzipping dataset')\n",
    "        file.extractall()\n",
    "    os.rename('HollywoodHeads','data')\n",
    "    !rm HollywoodHeads.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" basepath = 'data/Annotations/'\n",
    "corrupt_files = []\n",
    "i=0\n",
    "with os.scandir(basepath) as entries:\n",
    "    for entry in entries:\n",
    "        i+=1\n",
    "\n",
    "        if entry.is_file() and entry.name[-4:]=='.xml':\n",
    "            box_coordinates=[]\n",
    "            #print(entry.name)\n",
    "            tree = et.parse(entry)\n",
    "            root = tree.getroot()\n",
    "            for object in root.findall('object'):\n",
    "                try:\n",
    "                    xmin=float(object.find('bndbox').find('xmin').text)\n",
    "                    xmax=float(object.find('bndbox').find('xmax').text)\n",
    "\n",
    "                    ymin=float(object.find('bndbox').find('ymin').text)\n",
    "                    ymax=float(object.find('bndbox').find('ymax').text)\n",
    "                    box_coordinates = box_coordinates + [xmin, xmax, ymin, ymax]\n",
    "                except AttributeError:\n",
    "                    #print(entry.name)\n",
    "                    continue\n",
    "            #print(box_coordinates)\n",
    "            corrupt_coordinates = [coordinate for coordinate in box_coordinates if not coordinate.is_integer()]\n",
    "            if len(corrupt_coordinates)>0:\n",
    "                corrupt_files.append(entry.name)\n",
    "                #print(corrupt_files[-1])\n",
    "\n",
    "print(i)\n",
    "print(len(corrupt_files))\n",
    "            \n",
    "\n",
    "            \n",
    "             \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2 as tf\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import ImageReadMode\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.retinanet import RetinaNet, RetinaNetHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection import backbone_utils\n",
    "\n",
    "class HollywoodHeadDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None, mode='train') -> None:\n",
    "        super().__init__()\n",
    "        assert mode.lower() in ['train', 'test', 'val']\n",
    "        self.transforms = transforms\n",
    "        self.root = root\n",
    "\n",
    "        filename = mode.lower() + '.txt'\n",
    "        filepath = os.path.join(root,'Splits',filename)\n",
    "\n",
    "        with open(filepath,'r') as f:\n",
    "            img_names = f.readlines()\n",
    "        self.imgs = [img.strip('\\n') for img in img_names]\n",
    "\n",
    "        self.imgs_dir = os.path.join(root, 'JPEGImages')\n",
    "        self.annot_dir = os.path.join(root, 'Annotations')\n",
    "        #self.classes = ['background','head']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.imgs[idx]+'.jpeg'\n",
    "        image_path = os.path.join(self.imgs_dir,img_filename)\n",
    "\n",
    "        annot_filename = self.imgs[idx]+'.xml'\n",
    "        annot_file_path = os.path.join(self.annot_dir,annot_filename)\n",
    "\n",
    "        img = torchvision.io.read_image(image_path, ImageReadMode.RGB)        \n",
    "        boxes=[]\n",
    "        #labels=[]\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        for object in root.findall('object'):\n",
    "            #labels.append(self.classes.index(object.find('name').text))\n",
    "            if object.find('bndbox') is not None:\n",
    "                xmin=float(object.find('bndbox').find('xmin').text)\n",
    "                xmax=float(object.find('bndbox').find('xmax').text)\n",
    "\n",
    "                ymin=float(object.find('bndbox').find('ymin').text)\n",
    "                ymax=float(object.find('bndbox').find('ymax').text)\n",
    "\n",
    "                boxes.append([xmin,ymin, xmax,ymax])\n",
    "            # except AttributeError:\n",
    "            #     continue\n",
    "\n",
    "        #area = (boxes[:,2]-boxes[:,1])*(boxes[:,4]-boxes[:,3])\n",
    "        img = tv_tensors.Image(img)\n",
    "        #print(img.shape)\n",
    "        boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "        #iscrowd = torch.zeros(boxes.shape[0],dtype=torch.int64)\n",
    "        \n",
    "        #We only have one class\n",
    "        labels=torch.ones(boxes.shape[0], dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import random\n",
    "\n",
    "annot_dir = 'data/Annotations/'\n",
    "img_dir = 'data/JPEGImages/'\n",
    "for i in range(10):\n",
    "    img_name = random.choice(corrupt_files)[:-4]\n",
    "    annot_filepath = os.path.join(annot_dir, img_name+'.xml')\n",
    "    img_filepath = os.path.join(img_dir,img_name+'.jpeg')\n",
    "    img = torchvision.io.read_image(img_filepath,ImageReadMode.RGB)\n",
    "\n",
    "    boxes=[]\n",
    "    tree = et.parse(annot_filepath)\n",
    "    root = tree.getroot()\n",
    "    for object in root.findall('object'):\n",
    "            #labels.append(self.classes.index(object.find('name').text))\n",
    "            #TODO: There are floats in some of the xml files, which cannot always be parsed. Find a solution for this\n",
    "            try:\n",
    "                xmin=float(object.find('bndbox').find('xmin').text)\n",
    "                xmax=float(object.find('bndbox').find('xmax').text)\n",
    "\n",
    "                ymin=float(object.find('bndbox').find('ymin').text)\n",
    "                ymax=float(object.find('bndbox').find('ymax').text)\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "            except AttributeError:\n",
    "                 continue\n",
    "    img = tv_tensors.Image(img)\n",
    "    boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "    img = torchvision.utils.draw_bounding_boxes(img, boxes)\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    img.show() \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tf.Compose([tf.Resize((255,255)),\n",
    "                        tf.ToImage(),\n",
    "                        tf.ConvertImageDtype()])\n",
    "\n",
    "train_set = HollywoodHeadDataset(root='data', mode='train', transforms=transforms)\n",
    "test_set = HollywoodHeadDataset(root='data', mode='test', transforms=transforms)\n",
    "val_set = HollywoodHeadDataset(root='data', mode='val', transforms=transforms)\n",
    "\n",
    "print(f\"Length of train set = {len(train_set)}, test set = {len(test_set)}, validation set = {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train=512\n",
    "bs = 16\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for b in batch:\n",
    "        images.append(b[0])\n",
    "        targets.append(b[1])\n",
    "    images = torch.stack(images,dim=0)\n",
    "    return images, targets\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=bs_train, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, report_freq, device):\n",
    "    epoch_loss=[]\n",
    "    running_loss = 0.0\n",
    "    for j, (images, targets) in enumerate(dataloader):\n",
    "            batch_starttime = time.time()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            for target in targets:\n",
    "                 for key in target.keys():\n",
    "                      target[key].to(device)\n",
    "            print(f\"start calculating loss\")\n",
    "            batch_loss_dict = model(images.to(device),targets)\n",
    "            print(f\"Calculated loss dicationary, \"\n",
    "                  f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds.\")\n",
    "            batch_loss = sum(loss for loss in batch_loss_dict.values())\n",
    "            print(f\"Calculated the sum over losses, \"\n",
    "                  f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            batch_loss.backward()\n",
    "            print(f\"Finished backward calculation, \"\n",
    "                  f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            optimizer.step()\n",
    "            print(f\"Finished optimizer step, \"\n",
    "                  f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            epoch_loss.append(batch_loss.item())\n",
    "            running_loss+=batch_loss.item()\n",
    "            if j%report_freq==report_freq-1:\n",
    "                print(f\"Batch {j+1} finished, \"\n",
    "                      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \"\n",
    "                      f\"loss: {running_loss/report_freq}\")\n",
    "                running_loss = 0.0\n",
    "            else:\n",
    "                print(f\"Batch {j+1} finished, time = \"\n",
    "                      f\"{int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds\")\n",
    "    return epoch_loss\n",
    "\n",
    "def train_model(model,train_loader, test_loader, optimizer,no_of_epochs, report_freq, device='cpu'):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    accuracy = []\n",
    "    for epoch in range(no_of_epochs):\n",
    "        epoch_starttime = time.time()\n",
    "        print(f\"START TRAINING FOR EPOCH {epoch + 1}:\")\n",
    "        model.train(True)\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, report_freq, device)\n",
    "        train_loss+=epoch_loss\n",
    "            \n",
    "        running_vloss = 0.0\n",
    "        model.eval()\n",
    "        print(f\"Training for epoch {epoch+1} done, time = \"\n",
    "              f\"{int((time.time()-epoch_starttime)/60)} minutes {round((time.time()-epoch_starttime)%60,2)} seconds\")\n",
    "        with torch.no_grad():\n",
    "             for i, (vimages, vtargets) in enumerate(test_loader):\n",
    "                  vbatch_starttime = time.time()\n",
    "                  vloss_dict = model(vimages, vtargets)\n",
    "                  vloss = sum(loss for loss in vloss_dict.values())\n",
    "                  #correct = (torch.argmax(vpred, dim=1) == vlabels).type(torch.FloatTensor)\n",
    "                  val_loss.append(vloss.item())\n",
    "                  running_vloss+=vloss.item()\n",
    "                  #accuracy.append(correct.mean().item())\n",
    "                  print(f\"Completed validation for batch {i+1}, time = \"\n",
    "                        f\"{int((time.time()-vbatch_starttime)/60)} minutes {round((time.time()-vbatch_starttime)%60,2)}\"\n",
    "                        f\"seconds\")\n",
    "        \n",
    "\n",
    "        val_loss.append(running_vloss/(i+1))\n",
    "        train_loss+=epoch_loss\n",
    "        print(f\"Validation for epoch {epoch+1} done, time = \"\n",
    "              f\"{int((time.time()-epoch_starttime)/60)} minutes {round((time.time()-epoch_starttime)%60,2)} seconds, \"\n",
    "              f\"LOSS train {epoch_loss[-1]}, val: {val_loss[-1]}\")\n",
    "\n",
    "        \n",
    "    return train_loss, val_loss#, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [8, 16, 32, 64, 128])\n",
    "aspect_ratios = ((0.25, 0.5, 1.0, 1.5),)*len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(anchor_sizes,aspect_ratios)\n",
    "\n",
    "trainable_backbone_layers=1\n",
    "backbone = torchvision.models.resnet50(weights='DEFAULT')\n",
    "backbone = backbone_utils._resnet_fpn_extractor(backbone, \n",
    "                                                trainable_layers=trainable_backbone_layers,\n",
    "                                                returned_layers=[2,3,4],\n",
    "                                                extra_blocks=torchvision.ops.feature_pyramid_network.LastLevelP6P7(2046,256))\n",
    "\n",
    "backbone.out_channels=256\n",
    "head = RetinaNetHead(\n",
    "        backbone.out_channels,\n",
    "        anchor_generator.num_anchors_per_location()[0],\n",
    "        num_classes=2,\n",
    "        norm_layer=partial(nn.GroupNorm, 32))\n",
    "\n",
    "head.regression_head._loss_type = \"giou\"\n",
    "model = RetinaNet(backbone=backbone, \n",
    "                  num_classes=2, \n",
    "                  anchor_generator=anchor_generator,)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(len(params))\n",
    "optimizer = optim.SGD(params,lr=0.1,momentum=0.9)\n",
    "\n",
    "no_of_epochs = 1\n",
    "train_loss, test_loss = train_model(model, train_loader,test_loader, optimizer=optimizer, no_of_epochs=no_of_epochs, report_freq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
