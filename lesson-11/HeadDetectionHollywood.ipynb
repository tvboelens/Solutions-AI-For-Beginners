{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the data and unpack the zip file. The dataset is very large, so downloading takes quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    if not os.path.exists('HollywoodHeads.zip'):\n",
    "        print('Downloading dataset, this might take a while')\n",
    "        !wget https://www.di.ens.fr/willow/research/headdetection/release/HollywoodHeads.zip\n",
    "    with zipfile.ZipFile('HollywoodHeads.zip') as file:\n",
    "        print('Unzipping dataset')\n",
    "        file.extractall()\n",
    "    os.rename('HollywoodHeads','data')\n",
    "    !rm HollywoodHeads.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2 as tf\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import ImageReadMode\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.retinanet import RetinaNet, RetinaNetHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection import backbone_utils\n",
    "\n",
    "class HollywoodHeadDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None, mode='train') -> None:\n",
    "        super().__init__()\n",
    "        assert mode.lower() in ['train', 'test', 'val']\n",
    "        self.transforms = transforms\n",
    "        self.root = root\n",
    "\n",
    "        filename = mode.lower() + '.txt'\n",
    "        filepath = os.path.join(root,'Splits',filename)\n",
    "\n",
    "        with open(filepath,'r') as f:\n",
    "            img_names = f.readlines()\n",
    "        self.imgs = [img.strip('\\n') for img in img_names]\n",
    "\n",
    "        self.imgs_dir = os.path.join(root, 'JPEGImages')\n",
    "        self.annot_dir = os.path.join(root, 'Annotations')\n",
    "        #self.classes = ['background','head']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.imgs[idx]+'.jpeg'\n",
    "        image_path = os.path.join(self.imgs_dir,img_filename)\n",
    "\n",
    "        annot_filename = self.imgs[idx]+'.xml'\n",
    "        annot_file_path = os.path.join(self.annot_dir,annot_filename)\n",
    "\n",
    "        img = torchvision.io.read_image(image_path, ImageReadMode.RGB)        \n",
    "        boxes=[]\n",
    "        #labels=[]\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        for object in root.findall('object'):\n",
    "            #labels.append(self.classes.index(object.find('name').text))\n",
    "            if object.find('bndbox') is not None:\n",
    "                xmin=float(object.find('bndbox').find('xmin').text)\n",
    "                xmax=float(object.find('bndbox').find('xmax').text)\n",
    "\n",
    "                ymin=float(object.find('bndbox').find('ymin').text)\n",
    "                ymax=float(object.find('bndbox').find('ymax').text)\n",
    "\n",
    "                boxes.append([xmin,ymin, xmax,ymax])\n",
    "            # except AttributeError:\n",
    "            #     continue\n",
    "\n",
    "        #area = (boxes[:,2]-boxes[:,1])*(boxes[:,4]-boxes[:,3])\n",
    "        img = tv_tensors.Image(img)\n",
    "        #print(img.shape)\n",
    "        boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "        #iscrowd = torch.zeros(boxes.shape[0],dtype=torch.int64)\n",
    "        \n",
    "        #We only have one class\n",
    "        labels=torch.ones(boxes.shape[0], dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set = 216719, test set = 1302, validation set = 6719\n"
     ]
    }
   ],
   "source": [
    "transforms = tf.Compose([tf.Resize((255,255)),\n",
    "                        tf.ToImage(),\n",
    "                        tf.ConvertImageDtype()])\n",
    "\n",
    "train_set = HollywoodHeadDataset(root='data', mode='train', transforms=transforms)\n",
    "test_set = HollywoodHeadDataset(root='data', mode='test', transforms=transforms)\n",
    "val_set = HollywoodHeadDataset(root='data', mode='val', transforms=transforms)\n",
    "\n",
    "print(f\"Length of train set = {len(train_set)}, test set = {len(test_set)}, validation set = {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train=32\n",
    "bs = 16\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for b in batch:\n",
    "        images.append(b[0])\n",
    "        targets.append(b[1])\n",
    "    images = torch.stack(images,dim=0)\n",
    "    return images, targets\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=bs_train, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, report_freq, device):\n",
    "    epoch_loss=[]\n",
    "    running_loss = 0.0\n",
    "    for j, (images, targets) in enumerate(dataloader):\n",
    "            batch_starttime = time.time()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            for target in targets:\n",
    "                 for key in target.keys():\n",
    "                      target[key].to(device)\n",
    "            #print(f\"start calculating loss\")\n",
    "            batch_loss_dict = model(images.to(device),targets)\n",
    "            #print(f\"Calculated loss dictionary, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds.\")\n",
    "            batch_loss = sum(loss for loss in batch_loss_dict.values())\n",
    "            #print(f\"Calculated the sum over losses, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            batch_loss.backward()\n",
    "            #print(f\"Finished backward calculation, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            optimizer.step()\n",
    "            #print(f\"Finished optimizer step, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            epoch_loss.append(batch_loss.item())\n",
    "            running_loss+=batch_loss.item()\n",
    "            if j%report_freq==report_freq-1:\n",
    "                print(f\"Batch {j+1} finished, \"\n",
    "                      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \"\n",
    "                      f\"loss: {running_loss/report_freq}\")\n",
    "                running_loss = 0.0\n",
    "            else:\n",
    "                print(f\"Batch {j+1} finished, time = \"\n",
    "                      f\"{int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds\")\n",
    "    return epoch_loss\n",
    "\n",
    "def train_model(model,train_loader, test_loader, optimizer,no_of_epochs, report_freq, device='cpu'):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    accuracy = []\n",
    "    for epoch in range(no_of_epochs):\n",
    "        epoch_starttime = time.time()\n",
    "        print(f\"START TRAINING FOR EPOCH {epoch + 1}:\")\n",
    "        model.train(True)\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, report_freq, device)\n",
    "        train_loss+=epoch_loss\n",
    "            \n",
    "        running_vloss = 0.0\n",
    "        model.eval()\n",
    "        print(f\"Training for epoch {epoch+1} done, time = \"\n",
    "              f\"{int((time.time()-epoch_starttime)/60)} minutes {round((time.time()-epoch_starttime)%60,2)} seconds\")\n",
    "        with torch.no_grad():\n",
    "             for i, (vimages, vtargets) in enumerate(test_loader):\n",
    "                  vbatch_starttime = time.time()\n",
    "                  vloss_dict = model(vimages, vtargets)\n",
    "                  vloss = sum(loss for loss in vloss_dict.values())\n",
    "                  #correct = (torch.argmax(vpred, dim=1) == vlabels).type(torch.FloatTensor)\n",
    "                  val_loss.append(vloss.item())\n",
    "                  running_vloss+=vloss.item()\n",
    "                  #accuracy.append(correct.mean().item())\n",
    "                  print(f\"Completed validation for batch {i+1}, time = \"\n",
    "                        f\"{int((time.time()-vbatch_starttime)/60)} minutes {round((time.time()-vbatch_starttime)%60,2)}\"\n",
    "                        f\"seconds\")\n",
    "        \n",
    "\n",
    "        val_loss.append(running_vloss/(i+1))\n",
    "        train_loss+=epoch_loss\n",
    "        print(f\"Validation for epoch {epoch+1} done, time = \"\n",
    "              f\"{int((time.time()-epoch_starttime)/60)} minutes {round((time.time()-epoch_starttime)%60,2)} seconds, \"\n",
    "              f\"LOSS train {epoch_loss[-1]}, val: {val_loss[-1]}\")\n",
    "\n",
    "        \n",
    "    return train_loss, val_loss#, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetinaNet(\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-2): 3 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelP6P7(\n",
      "        (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (anchor_generator): AnchorGenerator()\n",
      "  (head): RetinaNetHead(\n",
      "    (classification_head): RetinaNetClassificationHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (regression_head): RetinaNetRegressionHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_reg): Conv2d(256, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [8, 16, 32, 64, 128])\n",
    "aspect_ratios = ((0.25, 0.5, 1.0, 1.5),)*len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(anchor_sizes,aspect_ratios)\n",
    "\n",
    "trainable_backbone_layers=3\n",
    "backbone = torchvision.models.resnet50(weights='DEFAULT')\n",
    "backbone = backbone_utils._resnet_fpn_extractor(backbone, \n",
    "                                                trainable_layers=trainable_backbone_layers,\n",
    "                                                returned_layers=[2,3,4],\n",
    "                                                extra_blocks=torchvision.ops.feature_pyramid_network.LastLevelP6P7(2048,256))\n",
    "\n",
    "backbone.out_channels=256\n",
    "head = RetinaNetHead(backbone.out_channels,\n",
    "                     anchor_generator.num_anchors_per_location()[0],\n",
    "                     num_classes=2,\n",
    "                     norm_layer=partial(nn.GroupNorm, 32))\n",
    "\n",
    "head.regression_head._loss_type = \"giou\"\n",
    "model = RetinaNet(backbone=backbone, \n",
    "                  num_classes=2, \n",
    "                  anchor_generator=anchor_generator,\n",
    "                  head=head)\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "START TRAINING FOR EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(len(params))\n",
    "optimizer = optim.SGD(params,lr=0.001,momentum=0.9)\n",
    "\n",
    "no_of_epochs = 3\n",
    "train_loss, test_loss = train_model(model, train_loader,test_loader, optimizer=optimizer, no_of_epochs=no_of_epochs, report_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "annot_dir = 'data/Annotations/'\n",
    "img_dir = 'data/JPEGImages/'\n",
    "imgs = val_set.imgs\n",
    "model.eval()\n",
    "for i in range(10):\n",
    "    img_name = random.choice(imgs)\n",
    "    annot_filepath = os.path.join(annot_dir, img_name+'.xml')\n",
    "    img_filepath = os.path.join(img_dir,img_name+'.jpeg')\n",
    "    img = torchvision.io.read_image(img_filepath,ImageReadMode.RGB)\n",
    "\n",
    "    img = tv_tensors.Image(img)\n",
    "    pred = model(img)\n",
    "    boxes = pred[\"boxes\"]\n",
    "\n",
    "\n",
    "    #boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "    img = torchvision.utils.draw_bounding_boxes(img, boxes)\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
