{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the data and unpack the zip file. The dataset is very large, so downloading takes quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from xml.etree import ElementTree as et\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    if not os.path.exists('HollywoodHeads.zip'):\n",
    "        print('Downloading dataset, this might take a while')\n",
    "        !wget https://www.di.ens.fr/willow/research/headdetection/release/HollywoodHeads.zip\n",
    "    with zipfile.ZipFile('HollywoodHeads.zip') as file:\n",
    "        print('Unzipping dataset')\n",
    "        file.extractall()\n",
    "    os.rename('HollywoodHeads','data')\n",
    "    !rm HollywoodHeads.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2 as tf\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.io import ImageReadMode\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.retinanet import RetinaNet, RetinaNetHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection import backbone_utils\n",
    "\n",
    "class HollywoodHeadDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None, mode='train') -> None:\n",
    "        super().__init__()\n",
    "        assert mode.lower() in ['train', 'test', 'val']\n",
    "        self.transforms = transforms\n",
    "        self.root = root\n",
    "\n",
    "        filename = mode.lower() + '.txt'\n",
    "        filepath = os.path.join(root,'Splits',filename)\n",
    "\n",
    "        with open(filepath,'r') as f:\n",
    "            img_names = f.readlines()\n",
    "        self.imgs = [img.strip('\\n') for img in img_names]\n",
    "\n",
    "        self.imgs_dir = os.path.join(root, 'JPEGImages')\n",
    "        self.annot_dir = os.path.join(root, 'Annotations')\n",
    "        #self.classes = ['background','head']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.imgs[idx]+'.jpeg'\n",
    "        image_path = os.path.join(self.imgs_dir,img_filename)\n",
    "\n",
    "        annot_filename = self.imgs[idx]+'.xml'\n",
    "        annot_file_path = os.path.join(self.annot_dir,annot_filename)\n",
    "\n",
    "        img = torchvision.io.read_image(image_path, ImageReadMode.RGB)        \n",
    "        boxes=[]\n",
    "        #labels=[]\n",
    "        tree = et.parse(annot_file_path)\n",
    "        root = tree.getroot()\n",
    "        for object in root.findall('object'):\n",
    "            #labels.append(self.classes.index(object.find('name').text))\n",
    "            if object.find('bndbox') is not None:\n",
    "                xmin=float(object.find('bndbox').find('xmin').text)\n",
    "                xmax=float(object.find('bndbox').find('xmax').text)\n",
    "\n",
    "                ymin=float(object.find('bndbox').find('ymin').text)\n",
    "                ymax=float(object.find('bndbox').find('ymax').text)\n",
    "\n",
    "                boxes.append([xmin,ymin, xmax,ymax])\n",
    "            # except AttributeError:\n",
    "            #     continue\n",
    "\n",
    "        #area = (boxes[:,2]-boxes[:,1])*(boxes[:,4]-boxes[:,3])\n",
    "        img = tv_tensors.Image(img)\n",
    "        \n",
    "        #If we have images without bounding boxes then transformations can lead to a runtime error. \n",
    "        #In this case we add a degenerate box to prevent the error.\n",
    "        if len(boxes)==0:\n",
    "            boxes = tv_tensors.BoundingBoxes([[0,0,0,0]], format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "            labels = torch.zeros(boxes.shape[0], dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "            #We only have one class\n",
    "            labels=torch.ones(boxes.shape[0], dtype=torch.int64)\n",
    "        \n",
    "        #iscrowd = torch.zeros(boxes.shape[0],dtype=torch.int64)\n",
    "        \n",
    "        \n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set = 216719, test set = 1302, validation set = 6719\n"
     ]
    }
   ],
   "source": [
    "transforms = tf.Compose([tf.Resize((255,255)),\n",
    "                        tf.ToImage(),\n",
    "                        tf.ConvertImageDtype()])\n",
    "\n",
    "train_set = HollywoodHeadDataset(root='data', mode='train', transforms=transforms)\n",
    "test_set = HollywoodHeadDataset(root='data', mode='test', transforms=transforms)\n",
    "val_set = HollywoodHeadDataset(root='data', mode='val', transforms=transforms)\n",
    "\n",
    "print(f\"Length of train set = {len(train_set)}, test set = {len(test_set)}, validation set = {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train=16\n",
    "bs = 16\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for b in batch:\n",
    "        images.append(b[0])\n",
    "        targets.append(b[1])\n",
    "    images = torch.stack(images,dim=0)\n",
    "    return images, targets\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=bs_train, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, report_freq, device):\n",
    "    epoch_loss=[]\n",
    "    running_loss = 0.0\n",
    "    for j, (images, targets) in enumerate(dataloader):\n",
    "            batch_starttime = time.time()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            for target in targets:\n",
    "                 for key in target.keys():\n",
    "                      target[key].to(device)\n",
    "            #print(f\"start calculating loss\")\n",
    "            batch_loss_dict = model(images.to(device),targets)\n",
    "            #print(f\"Calculated loss dictionary, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds.\")\n",
    "            batch_loss = sum(loss for loss in batch_loss_dict.values())\n",
    "            #print(f\"Calculated the sum over losses, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            batch_loss.backward()\n",
    "            #print(f\"Finished backward calculation, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            optimizer.step()\n",
    "            #print(f\"Finished optimizer step, \"\n",
    "            #      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \")\n",
    "            epoch_loss.append(batch_loss.item())\n",
    "            running_loss+=batch_loss.item()\n",
    "            if j%report_freq==report_freq-1:\n",
    "                print(f\"Batch {j+1} finished, \"\n",
    "                      f\"time = {int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds, \"\n",
    "                      f\"loss: {running_loss/report_freq}\")\n",
    "                running_loss = 0.0\n",
    "            else:\n",
    "                print(f\"Batch {j+1} finished, time = \"\n",
    "                      f\"{int((time.time()-batch_starttime)/60)} minutes {round((time.time()-batch_starttime)%60,2)} seconds\")\n",
    "    return epoch_loss\n",
    "\n",
    "def train_model(model,train_loader, test_loader, optimizer,no_of_epochs, report_freq, device='cpu'):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    accuracy = []\n",
    "    for epoch in range(no_of_epochs):\n",
    "        epoch_starttime = time.time()\n",
    "        print(f\"START TRAINING FOR EPOCH {epoch + 1}:\")\n",
    "        model.train(True)\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, report_freq, device)\n",
    "        train_loss+=epoch_loss\n",
    "            \n",
    "        running_vloss = 0.0\n",
    "        model.eval()\n",
    "        print(f\"Training for epoch {epoch+1} done, time = \"\n",
    "              f\"{int((time.time()-epoch_starttime)/60)} minutes {round((time.time()-epoch_starttime)%60,2)} seconds\")\n",
    "        with torch.no_grad():\n",
    "             for i, (vimages, vtargets) in enumerate(test_loader):\n",
    "                  vbatch_starttime = time.time()\n",
    "                  vloss_dict = model(vimages, vtargets)\n",
    "                  vloss = sum(loss for loss in vloss_dict.values())\n",
    "                  #correct = (torch.argmax(vpred, dim=1) == vlabels).type(torch.FloatTensor)\n",
    "                  val_loss.append(vloss.item())\n",
    "                  running_vloss+=vloss.item()\n",
    "                  #accuracy.append(correct.mean().item())\n",
    "                  print(f\"Completed validation for batch {i+1}, time = \"\n",
    "                        f\"{int((time.time()-vbatch_starttime)/60)} minutes {round((time.time()-vbatch_starttime)%60,2)}\"\n",
    "                        f\"seconds\")\n",
    "        \n",
    "\n",
    "        val_loss.append(running_vloss/(i+1))\n",
    "        train_loss+=epoch_loss\n",
    "        print(f\"Validation for epoch {epoch+1} done, time = \"\n",
    "              f\"{int((time.time()-epoch_starttime)/60)} minutes {round((time.time()-epoch_starttime)%60,2)} seconds, \"\n",
    "              f\"LOSS train {epoch_loss[-1]}, val: {val_loss[-1]}\")\n",
    "\n",
    "        \n",
    "    return train_loss, val_loss#, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetinaNet(\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-2): 3 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelP6P7(\n",
      "        (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (anchor_generator): AnchorGenerator()\n",
      "  (head): RetinaNetHead(\n",
      "    (classification_head): RetinaNetClassificationHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (regression_head): RetinaNetRegressionHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (bbox_reg): Conv2d(256, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [8, 16, 32, 64, 128])\n",
    "aspect_ratios = ((0.25, 0.5, 1.0, 1.5),)*len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(anchor_sizes,aspect_ratios)\n",
    "\n",
    "trainable_backbone_layers=3\n",
    "backbone = torchvision.models.resnet50(weights='DEFAULT')\n",
    "backbone = backbone_utils._resnet_fpn_extractor(backbone, \n",
    "                                                trainable_layers=trainable_backbone_layers,\n",
    "                                                returned_layers=[2,3,4],\n",
    "                                                extra_blocks=torchvision.ops.feature_pyramid_network.LastLevelP6P7(2048,256))\n",
    "\n",
    "backbone.out_channels=256\n",
    "head = RetinaNetHead(backbone.out_channels,\n",
    "                     anchor_generator.num_anchors_per_location()[0],\n",
    "                     num_classes=2,\n",
    "                     norm_layer=partial(nn.GroupNorm, 32))\n",
    "\n",
    "head.regression_head._loss_type = \"giou\"\n",
    "model = RetinaNet(backbone=backbone, \n",
    "                  num_classes=2, \n",
    "                  anchor_generator=anchor_generator,\n",
    "                  head=head)\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING FOR EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 finished, time = 7 minutes 48.49 seconds, loss: 2.0687620639801025\n",
      "Batch 2 finished, time = 10 minutes 13.93 seconds, loss: 1.945216178894043\n",
      "Batch 3 finished, time = 6 minutes 53.73 seconds, loss: 1.8669993877410889\n",
      "Batch 4 finished, time = 6 minutes 57.22 seconds, loss: 1.916785478591919\n",
      "Batch 5 finished, time = 6 minutes 25.76 seconds, loss: 1.8063689470291138\n",
      "Batch 6 finished, time = 6 minutes 27.26 seconds, loss: 1.7474324703216553\n",
      "Batch 7 finished, time = 6 minutes 42.75 seconds, loss: 1.6213264465332031\n",
      "Batch 8 finished, time = 6 minutes 41.9 seconds, loss: 1.6300733089447021\n",
      "Batch 9 finished, time = 8 minutes 15.63 seconds, loss: 1.732025384902954\n",
      "Batch 10 finished, time = 7 minutes 20.97 seconds, loss: 1.4864250421524048\n",
      "Batch 11 finished, time = 7 minutes 12.55 seconds, loss: 1.5628418922424316\n",
      "Batch 12 finished, time = 6 minutes 28.89 seconds, loss: 1.5288913249969482\n",
      "Batch 13 finished, time = 6 minutes 9.25 seconds, loss: 1.5471237897872925\n",
      "Batch 14 finished, time = 5 minutes 50.25 seconds, loss: 1.4293776750564575\n",
      "Batch 15 finished, time = 6 minutes 49.31 seconds, loss: 1.2711992263793945\n",
      "Batch 16 finished, time = 7 minutes 43.96 seconds, loss: 1.3704180717468262\n",
      "Batch 17 finished, time = 6 minutes 16.07 seconds, loss: 1.3518145084381104\n",
      "Batch 18 finished, time = 5 minutes 35.05 seconds, loss: 1.3653286695480347\n",
      "Batch 19 finished, time = 5 minutes 36.72 seconds, loss: 1.4348571300506592\n",
      "Batch 20 finished, time = 5 minutes 30.41 seconds, loss: 1.347872018814087\n",
      "Batch 21 finished, time = 5 minutes 33.28 seconds, loss: 1.3023736476898193\n",
      "Batch 22 finished, time = 5 minutes 29.54 seconds, loss: 1.345245361328125\n",
      "Batch 23 finished, time = 5 minutes 34.37 seconds, loss: 1.2898266315460205\n",
      "Batch 24 finished, time = 5 minutes 29.63 seconds, loss: 1.2850415706634521\n",
      "Batch 25 finished, time = 5 minutes 27.97 seconds, loss: 1.494318962097168\n",
      "Batch 26 finished, time = 5 minutes 28.12 seconds, loss: 1.2596831321716309\n",
      "Batch 27 finished, time = 5 minutes 30.73 seconds, loss: 1.4158871173858643\n",
      "Batch 28 finished, time = 5 minutes 32.9 seconds, loss: 1.3929424285888672\n",
      "Batch 29 finished, time = 5 minutes 25.58 seconds, loss: 1.501706600189209\n",
      "Batch 30 finished, time = 5 minutes 30.44 seconds, loss: 1.40342116355896\n",
      "Batch 31 finished, time = 6 minutes 11.18 seconds, loss: 1.4779140949249268\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(params,lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m no_of_epochs \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_loss, test_loss \u001b[39m=\u001b[39m train_model(model, train_loader,test_loader, optimizer\u001b[39m=\u001b[39;49moptimizer, no_of_epochs\u001b[39m=\u001b[39;49mno_of_epochs, report_freq\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSTART TRAINING FOR EPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m train_epoch(model, train_loader, optimizer, report_freq, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m train_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mepoch_loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[1;32m/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m epoch_loss\u001b[39m=\u001b[39m[]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m j, (images, targets) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         batch_starttime \u001b[39m=\u001b[39;49m time\u001b[39m.\u001b[39;49mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         optimizer\u001b[39m.\u001b[39;49mzero_grad(set_to_none\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m target[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([idx])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(img, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vincentboelens/Documents/repositories/AI-For-Beginners/solutions/lesson-11/HeadDetectionHollywood.ipynb#X11sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img, target\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py:53\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     51\u001b[0m needs_unpacking \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(inputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 53\u001b[0m     outputs \u001b[39m=\u001b[39m transform(\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     54\u001b[0m     inputs \u001b[39m=\u001b[39m outputs \u001b[39mif\u001b[39;00m needs_unpacking \u001b[39melse\u001b[39;00m (outputs,)\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:50\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;49;00m needs_transform \u001b[39melse\u001b[39;49;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;49;00m (inpt, needs_transform) \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[1;32m     46\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params(\n\u001b[1;32m     47\u001b[0m     [inpt \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;00m needs_transform]\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(inpt, params) \u001b[39mif\u001b[39;00m needs_transform \u001b[39melse\u001b[39;00m inpt\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/_geometry.py:159\u001b[0m, in \u001b[0;36mResize._transform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, inpt: Any, params: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_kernel(\n\u001b[1;32m    160\u001b[0m         F\u001b[39m.\u001b[39;49mresize,\n\u001b[1;32m    161\u001b[0m         inpt,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize,\n\u001b[1;32m    163\u001b[0m         interpolation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation,\n\u001b[1;32m    164\u001b[0m         max_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size,\n\u001b[1;32m    165\u001b[0m         antialias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias,\n\u001b[1;32m    166\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py:35\u001b[0m, in \u001b[0;36mTransform._call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_kernel\u001b[39m(\u001b[39mself\u001b[39m, functional: Callable, inpt: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     34\u001b[0m     kernel \u001b[39m=\u001b[39m _get_kernel(functional, \u001b[39mtype\u001b[39m(inpt), allow_passthrough\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m kernel(inpt, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_geometry.py:352\u001b[0m, in \u001b[0;36m_resize_bounding_boxes_dispatch\u001b[0;34m(inpt, size, max_size, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39m@_register_kernel_internal\u001b[39m(resize, tv_tensors\u001b[39m.\u001b[39mBoundingBoxes, tv_tensor_wrapper\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_resize_bounding_boxes_dispatch\u001b[39m(\n\u001b[1;32m    350\u001b[0m     inpt: tv_tensors\u001b[39m.\u001b[39mBoundingBoxes, size: List[\u001b[39mint\u001b[39m], max_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m    351\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m tv_tensors\u001b[39m.\u001b[39mBoundingBoxes:\n\u001b[0;32m--> 352\u001b[0m     output, canvas_size \u001b[39m=\u001b[39m resize_bounding_boxes(\n\u001b[1;32m    353\u001b[0m         inpt\u001b[39m.\u001b[39;49mas_subclass(torch\u001b[39m.\u001b[39;49mTensor), inpt\u001b[39m.\u001b[39;49mcanvas_size, size, max_size\u001b[39m=\u001b[39;49mmax_size\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m tv_tensors\u001b[39m.\u001b[39mwrap(output, like\u001b[39m=\u001b[39minpt, canvas_size\u001b[39m=\u001b[39mcanvas_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_geometry.py:343\u001b[0m, in \u001b[0;36mresize_bounding_boxes\u001b[0;34m(bounding_boxes, canvas_size, size, max_size)\u001b[0m\n\u001b[1;32m    340\u001b[0m h_ratio \u001b[39m=\u001b[39m new_height \u001b[39m/\u001b[39m old_height\n\u001b[1;32m    341\u001b[0m ratios \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([w_ratio, h_ratio, w_ratio, h_ratio], device\u001b[39m=\u001b[39mbounding_boxes\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 343\u001b[0m     bounding_boxes\u001b[39m.\u001b[39;49mmul(ratios)\u001b[39m.\u001b[39mto(bounding_boxes\u001b[39m.\u001b[39mdtype),\n\u001b[1;32m    344\u001b[0m     (new_height, new_width),\n\u001b[1;32m    345\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "#print(len(params))\n",
    "optimizer = optim.SGD(params,lr=0.001,momentum=0.9)\n",
    "\n",
    "no_of_epochs = 3\n",
    "train_loss, test_loss = train_model(model, train_loader,test_loader, optimizer=optimizer, no_of_epochs=no_of_epochs, report_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "annot_dir = 'data/Annotations/'\n",
    "img_dir = 'data/JPEGImages/'\n",
    "imgs = val_set.imgs\n",
    "model.eval()\n",
    "for i in range(10):\n",
    "    img_name = random.choice(imgs)\n",
    "    annot_filepath = os.path.join(annot_dir, img_name+'.xml')\n",
    "    img_filepath = os.path.join(img_dir,img_name+'.jpeg')\n",
    "    img = torchvision.io.read_image(img_filepath,ImageReadMode.RGB)\n",
    "\n",
    "    img = tv_tensors.Image(img)\n",
    "    pred = model(img)\n",
    "    boxes = pred[\"boxes\"]\n",
    "\n",
    "\n",
    "    #boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=img.shape[-2:])\n",
    "    img = torchvision.utils.draw_bounding_boxes(img, boxes)\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
